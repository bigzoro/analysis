package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"strings"
	"sync"
	"time"

	"analysis/internal/config"
	pdb "analysis/internal/db"
	"analysis/internal/netutil"
	"analysis/internal/server"

	"gorm.io/gorm"
)

// ===== æ·±åº¦åŒæ­¥å™¨ =====

// DepthSyncerConfig æ·±åº¦åŒæ­¥å™¨é…ç½®
type DepthSyncerConfig struct {
	SpotSymbols    []string // ç°è´§äº¤æ˜“å¯¹
	FuturesSymbols []string // æœŸè´§äº¤æ˜“å¯¹
}

// buildDepthSyncerConfig æ„å»ºæ·±åº¦åŒæ­¥å™¨é…ç½®
func (s *DepthSyncer) buildDepthSyncerConfig() DepthSyncerConfig {
	config := DepthSyncerConfig{}

	// ä¼˜å…ˆä»æ•°æ®åº“è·å–å„å¸‚åœºçš„æœ‰æ•ˆäº¤æ˜“å¯¹ï¼Œé¿å…ä½¿ç”¨åŒ…å«æ— æ•ˆç¬¦å·çš„å…¨å±€é…ç½®
	if spotSymbols, err := pdb.GetUSDTTradingPairsByMarket(s.db, "spot"); err == nil {
		// è¿‡æ»¤æ‰Redisç¼“å­˜ä¸­æ ‡è®°ä¸ºæ— æ•ˆçš„ç¬¦å·
		config.SpotSymbols = s.filterOutInvalidSymbols(spotSymbols, "spot")
		log.Printf("[DepthSyncer] âœ… Loaded %d spot symbols from database (%d after filtering invalid)",
			len(spotSymbols), len(config.SpotSymbols))
	} else {
		log.Printf("[DepthSyncer] âš ï¸ Failed to get spot symbols: %v", err)
		// å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ä»é…ç½®ä¸­è·å–
		if len(s.config.Symbols) > 0 {
			config.SpotSymbols = s.config.Symbols
			log.Printf("[DepthSyncer] ğŸ”„ Using configured symbols as fallback for spot: %d symbols", len(config.SpotSymbols))
		}
	}

	if futuresSymbols, err := pdb.GetUSDTTradingPairsByMarket(s.db, "futures"); err == nil {
		// è¿‡æ»¤æ‰Redisç¼“å­˜ä¸­æ ‡è®°ä¸ºæ— æ•ˆçš„ç¬¦å·
		config.FuturesSymbols = s.filterOutInvalidSymbols(futuresSymbols, "futures")
		log.Printf("[DepthSyncer] âœ… Loaded %d futures symbols from database (%d after filtering invalid)",
			len(futuresSymbols), len(config.FuturesSymbols))
	} else {
		log.Printf("[DepthSyncer] âš ï¸ Failed to get futures symbols: %v", err)
		// å¦‚æœæ•°æ®åº“æŸ¥è¯¢å¤±è´¥ï¼Œå°è¯•ä»é…ç½®ä¸­è·å–
		if len(s.config.Symbols) > 0 {
			config.FuturesSymbols = s.config.Symbols
			log.Printf("[DepthSyncer] ğŸ”„ Using configured symbols as fallback for futures: %d symbols", len(config.FuturesSymbols))
		}
	}

	return config
}

// isSymbolInvalid æ£€æŸ¥äº¤æ˜“å¯¹æ˜¯å¦ä¸ºæ— æ•ˆç¬¦å·
func (s *DepthSyncer) isSymbolInvalid(symbol, kind string) bool {
	// é¦–å…ˆæ£€æŸ¥Redisç¼“å­˜ï¼ˆè·¨æœåŠ¡å…±äº«ï¼‰
	if s.redisCache != nil && s.redisCache.IsInvalid(symbol, kind) {
		return true
	}

	// DepthSynceræ²¡æœ‰æœ¬åœ°å†…å­˜ç¼“å­˜ï¼Œç›´æ¥è¿”å›false
	return false
}

// filterOutInvalidSymbols è¿‡æ»¤æ‰Redisç¼“å­˜ä¸­æ ‡è®°ä¸ºæ— æ•ˆçš„ç¬¦å·
func (s *DepthSyncer) filterOutInvalidSymbols(symbols []string, marketType string) []string {
	if len(symbols) == 0 {
		return symbols
	}

	var validSymbols []string
	for _, symbol := range symbols {
		if !s.isSymbolInvalid(symbol, marketType) {
			validSymbols = append(validSymbols, symbol)
		} else {
			log.Printf("[DepthSyncer] ğŸ—‘ï¸ Filtered out invalid symbol: %s %s", symbol, marketType)
		}
	}

	return validSymbols
}

// filterConfiguredSymbols è¿‡æ»¤å‡ºé…ç½®ä¸­å­˜åœ¨çš„äº¤æ˜“å¯¹
func (s *DepthSyncer) filterConfiguredSymbols(configured, available []string) []string {
	configMap := make(map[string]bool)
	for _, symbol := range configured {
		configMap[symbol] = true
	}

	var result []string
	for _, symbol := range available {
		if configMap[symbol] {
			result = append(result, symbol)
		}
	}

	return result
}

// syncMarketDepth åŒæ­¥æŒ‡å®šå¸‚åœºçš„æ·±åº¦æ•°æ®
func (s *DepthSyncer) syncMarketDepth(ctx context.Context, symbols []string, marketType string) (int, int) {
	if len(symbols) == 0 {
		return 0, 0
	}

	var symbolsToSync []string

	// ğŸ”„ å¢é‡åŒæ­¥ï¼šåªåŒæ­¥éœ€è¦æ›´æ–°çš„äº¤æ˜“å¯¹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
	if s.config.EnableIncrementalSync {
		log.Printf("[DepthSyncer] ğŸ”„ Incremental sync enabled for %s market, checking for outdated symbols...", marketType)
		filteredSymbols, err := s.getSymbolsNeedingDepthSyncByMarket(symbols, marketType)
		if err != nil {
			log.Printf("[DepthSyncer] âš ï¸ Failed to determine symbols needing %s depth sync: %v, falling back to full sync", marketType, err)
			symbolsToSync = symbols // å›é€€åˆ°å…¨é‡åŒæ­¥
		} else {
			symbolsToSync = filteredSymbols
		}
	} else {
		log.Printf("[DepthSyncer] ğŸ”„ Incremental sync disabled for %s market, performing full sync...", marketType)
		symbolsToSync = symbols // å…¨é‡åŒæ­¥
	}

	log.Printf("[DepthSyncer] ğŸ¯ Starting %s market depth sync for %d/%d symbols",
		marketType, len(symbolsToSync), len(symbols))

	// å¦‚æœæ²¡æœ‰éœ€è¦åŒæ­¥çš„äº¤æ˜“å¯¹ï¼Œè·³è¿‡åŒæ­¥
	if len(symbolsToSync) == 0 {
		log.Printf("[DepthSyncer] âœ… All %s market symbols are up-to-date, skipping depth sync", marketType)
		return 0, 0
	}

	// ä¸´æ—¶ä¿å­˜åŸå§‹symbolså¹¶è®¾ç½®æ–°çš„symbols
	originalSymbols := s.config.Symbols
	s.config.Symbols = symbolsToSync                      // åªåŒæ­¥éœ€è¦æ›´æ–°çš„äº¤æ˜“å¯¹
	defer func() { s.config.Symbols = originalSymbols }() // æ¢å¤åŸå§‹é…ç½®

	updates := 0
	errors := 0

	for i, symbol := range symbolsToSync {
		// è·å–è®¢å•ç°¿æ·±åº¦
		if err := s.syncOrderBookDepth(ctx, symbol, marketType); err != nil {
			log.Printf("[DepthSyncer] âŒ Failed to sync %s depth for %s: %v", marketType, symbol, err)
			errors++
		} else {
			log.Printf("[DepthSyncer] âœ… Synced %s depth for %s", marketType, symbol)
			updates++
		}

		// æ·»åŠ å°å»¶è¿Ÿé¿å…APIé™æµï¼Œæ¯å¤„ç†10ä¸ªäº¤æ˜“å¯¹åå¢åŠ å»¶è¿Ÿ
		if (i+1)%10 == 0 && i < len(symbolsToSync)-1 {
			time.Sleep(200 * time.Millisecond)
			log.Printf("[DepthSyncer] Added delay after processing %d %s market symbols to prevent API rate limiting", i+1, marketType)
		}
	}

	log.Printf("[DepthSyncer] ğŸ“Š %s market depth sync completed: %d updates, %d errors",
		marketType, updates, errors)

	return updates, errors
}

// getSymbolsNeedingDepthSyncByMarket æŒ‰å¸‚åœºè·å–éœ€è¦æ·±åº¦åŒæ­¥çš„äº¤æ˜“å¯¹
func (s *DepthSyncer) getSymbolsNeedingDepthSyncByMarket(allSymbols []string, marketType string) ([]string, error) {
	if len(allSymbols) == 0 {
		return allSymbols, nil
	}

	// è®¾ç½®æ·±åº¦æ•°æ®è¿‡æœŸæ—¶é—´ï¼ˆä¾‹å¦‚10åˆ†é’Ÿï¼‰
	maxDataAge := 10 * time.Minute
	cutoffTime := time.Now().Add(-maxDataAge)

	// ä½¿ç”¨é€šé“æ”¶é›†ç»“æœ
	type checkResult struct {
		symbol    string
		needsSync bool
	}

	resultChan := make(chan checkResult, len(allSymbols))
	var wg sync.WaitGroup

	// é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…æ•°æ®åº“å‹åŠ›è¿‡å¤§
	maxConcurrency := 10
	semaphore := make(chan struct{}, maxConcurrency)

	// å¹¶å‘æ£€æŸ¥æ¯ä¸ªäº¤æ˜“å¯¹
	for _, symbol := range allSymbols {
		wg.Add(1)
		go func(sym string) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// æ£€æŸ¥è¯¥äº¤æ˜“å¯¹æ˜¯å¦éœ€è¦åŒæ­¥
			needsSync := s.checkSymbolNeedsDepthSyncByMarket(sym, marketType, cutoffTime)
			resultChan <- checkResult{symbol: sym, needsSync: needsSync}
		}(symbol)
	}

	// ç­‰å¾…æ‰€æœ‰æ£€æŸ¥å®Œæˆ
	go func() {
		wg.Wait()
		close(resultChan)
	}()

	// æ”¶é›†ç»“æœ
	var symbolsToSync []string
	for result := range resultChan {
		if result.needsSync {
			symbolsToSync = append(symbolsToSync, result.symbol)
		}
	}

	// å¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼Œè‡³å°‘åŒæ­¥å‡ ä¸ªæ ¸å¿ƒäº¤æ˜“å¯¹
	if len(symbolsToSync) == 0 && len(allSymbols) > 0 {
		coreSymbols := []string{"BTCUSDT", "ETHUSDT"}
		for _, coreSymbol := range coreSymbols {
			if s.containsString(allSymbols, coreSymbol) {
				symbolsToSync = append(symbolsToSync, coreSymbol)
			}
		}
	}

	log.Printf("[DepthSyncer] ğŸ”„ %s market incremental sync: %d/%d symbols need depth updating",
		marketType, len(symbolsToSync), len(allSymbols))

	return symbolsToSync, nil
}

// checkSymbolNeedsDepthSyncByMarket æ£€æŸ¥å•ä¸ªäº¤æ˜“å¯¹åœ¨æŒ‡å®šå¸‚åœºæ˜¯å¦éœ€è¦æ·±åº¦åŒæ­¥
func (s *DepthSyncer) checkSymbolNeedsDepthSyncByMarket(symbol, marketType string, cutoffTime time.Time) bool {
	var result struct {
		LastUpdateTime time.Time `json:"last_update_time"`
		RecordCount    int       `json:"record_count"`
	}

	// æŸ¥è¯¢è¯¥äº¤æ˜“å¯¹è¯¥å¸‚åœºçš„æœ€æ–°æ·±åº¦æ—¶é—´
	query := `
		SELECT MAX(created_at) as last_update_time, COUNT(*) as record_count
		FROM binance_order_book_depth
		WHERE symbol = ? AND market_type = ? AND created_at >= ?
	`

	err := s.db.Raw(query, symbol, marketType, cutoffTime).Scan(&result).Error
	if err != nil {
		// æŸ¥è¯¢å¤±è´¥ï¼Œå‡è®¾éœ€è¦åŒæ­¥
		log.Printf("[DepthSyncer] æŸ¥è¯¢ %s %s æ·±åº¦å¤±è´¥: %v", symbol, marketType, err)
		return true
	}

	// å¦‚æœæ²¡æœ‰è®°å½•æˆ–è®°å½•æ•°å¤ªå°‘ï¼Œéœ€è¦åŒæ­¥
	if result.LastUpdateTime.IsZero() || result.RecordCount < 3 {
		return true
	}

	// å¦‚æœæœ€æ–°æ·±åº¦æ—¶é—´å¤ªæ—§ï¼Œéœ€è¦åŒæ­¥
	if result.LastUpdateTime.Before(cutoffTime) {
		return true
	}

	return false
}

// containsString æ£€æŸ¥å­—ç¬¦ä¸²åˆ‡ç‰‡æ˜¯å¦åŒ…å«æŒ‡å®šå­—ç¬¦ä¸²
func (s *DepthSyncer) containsString(slice []string, item string) bool {
	for _, s := range slice {
		if s == item {
			return true
		}
	}
	return false
}

type DepthSyncer struct {
	db     *gorm.DB
	cfg    *config.Config
	config *DataSyncConfig

	// Redisç¼“å­˜ï¼Œç”¨äºè·¨æœåŠ¡å…±äº«æ— æ•ˆç¬¦å·
	redisCache *RedisInvalidSymbolCache

	stats struct {
		mu                 sync.RWMutex
		totalSyncs         int64
		successfulSyncs    int64
		failedSyncs        int64
		lastSyncTime       time.Time
		totalDepthUpdates  int64
		totalAPICalls      int64
		successfulAPICalls int64
		totalLatency       time.Duration
	}
}

func NewDepthSyncer(db *gorm.DB, cfg *config.Config, config *DataSyncConfig, redisCache *RedisInvalidSymbolCache) *DepthSyncer {
	return &DepthSyncer{
		db:         db,
		cfg:        cfg,
		config:     config,
		redisCache: redisCache,
	}
}

func (s *DepthSyncer) Name() string {
	return "depth"
}

// getSymbolsNeedingDepthSync å¢é‡åŒæ­¥ï¼šè·å–éœ€è¦åŒæ­¥å¸‚åœºæ·±åº¦çš„äº¤æ˜“å¯¹
// è¶…ä¼˜åŒ–ç‰ˆæœ¬ï¼šå¹¶å‘æŸ¥è¯¢ï¼Œå¸‚åœºæ·±åº¦å˜åŒ–å¿«ï¼Œéœ€è¦å¿«é€Ÿæ£€æŸ¥
func (s *DepthSyncer) getSymbolsNeedingDepthSync(allSymbols []string) ([]string, error) {
	if len(allSymbols) == 0 {
		return allSymbols, nil
	}

	// è®¾ç½®å¸‚åœºæ·±åº¦æ•°æ®è¿‡æœŸæ—¶é—´ï¼ˆ30ç§’ï¼Œæ·±åº¦æ•°æ®å˜åŒ–å¾ˆå¿«ï¼‰
	maxDataAge := 30 * time.Second
	cutoffTime := time.Now().Add(-maxDataAge)

	// ä½¿ç”¨é€šé“æ”¶é›†ç»“æœ
	type checkResult struct {
		symbol    string
		needsSync bool
	}

	resultChan := make(chan checkResult, len(allSymbols))
	var wg sync.WaitGroup

	// é™åˆ¶å¹¶å‘æ•°é‡ï¼Œé¿å…æ•°æ®åº“å‹åŠ›è¿‡å¤§
	maxConcurrency := 20 // æ·±åº¦æ£€æŸ¥å¹¶å‘å¯ä»¥æ›´é«˜ï¼Œå› ä¸ºæŸ¥è¯¢å¾ˆç®€å•
	semaphore := make(chan struct{}, maxConcurrency)

	// å¹¶å‘æ£€æŸ¥æ¯ä¸ªäº¤æ˜“å¯¹
	for _, symbol := range allSymbols {
		wg.Add(1)
		go func(sym string) {
			defer wg.Done()

			// è·å–ä¿¡å·é‡
			semaphore <- struct{}{}
			defer func() { <-semaphore }()

			// æ£€æŸ¥è¯¥äº¤æ˜“å¯¹æ˜¯å¦éœ€è¦åŒæ­¥
			needsSync := s.checkSymbolNeedsDepthSync(sym, cutoffTime)
			resultChan <- checkResult{symbol: sym, needsSync: needsSync}
		}(symbol)
	}

	// ç­‰å¾…æ‰€æœ‰æ£€æŸ¥å®Œæˆ
	go func() {
		wg.Wait()
		close(resultChan)
	}()

	// æ”¶é›†ç»“æœ
	var symbolsToSync []string
	for result := range resultChan {
		if result.needsSync {
			symbolsToSync = append(symbolsToSync, result.symbol)
		}
	}

	// å¦‚æœæ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ€æ–°çš„ï¼Œè‡³å°‘åŒæ­¥å‡ ä¸ªæ ¸å¿ƒäº¤æ˜“å¯¹
	if len(symbolsToSync) == 0 && len(allSymbols) > 0 {
		coreSymbols := []string{"BTCUSDT", "ETHUSDT"}
		for _, coreSymbol := range coreSymbols {
			if s.containsString(allSymbols, coreSymbol) {
				symbolsToSync = append(symbolsToSync, coreSymbol)
			}
		}
	}

	log.Printf("[DepthSyncer] ğŸ”„ Incremental sync: %d/%d symbols need depth updating",
		len(symbolsToSync), len(allSymbols))

	return symbolsToSync, nil
}

// checkSymbolNeedsDepthSync æ£€æŸ¥å•ä¸ªäº¤æ˜“å¯¹æ˜¯å¦éœ€è¦æ·±åº¦åŒæ­¥
func (s *DepthSyncer) checkSymbolNeedsDepthSync(symbol string, cutoffTime time.Time) bool {
	var result struct {
		LastUpdate time.Time `json:"last_update"`
	}

	// æŸ¥è¯¢è¯¥äº¤æ˜“å¯¹çš„æœ€æ–°æ·±åº¦æ›´æ–°æ—¶é—´
	query := `
		SELECT MAX(created_at) as last_update
		FROM binance_order_book_depth
		WHERE symbol = ?
	`

	err := s.db.Raw(query, symbol).Scan(&result).Error
	if err != nil {
		// æŸ¥è¯¢å¤±è´¥ï¼Œå‡è®¾éœ€è¦åŒæ­¥
		return true
	}

	// å¦‚æœæ²¡æœ‰è®°å½•ï¼Œéœ€è¦åŒæ­¥
	if result.LastUpdate.IsZero() {
		return true
	}

	// å¦‚æœæœ€æ–°è®°å½•å¤ªæ—§ï¼Œéœ€è¦åŒæ­¥
	if result.LastUpdate.Before(cutoffTime) {
		return true
	}

	return false
}

func (s *DepthSyncer) Start(ctx context.Context, interval time.Duration) {
	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	log.Printf("[DepthSyncer] Started with interval: %v", interval)

	for {
		select {
		case <-ctx.Done():
			log.Printf("[DepthSyncer] Stopped")
			return
		case <-ticker.C:
			if err := s.Sync(ctx); err != nil {
				log.Printf("[DepthSyncer] Sync failed: %v", err)
			}
		}
	}
}

func (s *DepthSyncer) Stop() {
	log.Printf("[DepthSyncer] Stop signal received")
}

func (s *DepthSyncer) Sync(ctx context.Context) error {
	s.stats.mu.Lock()
	s.stats.totalSyncs++
	syncStartTime := time.Now()
	s.stats.lastSyncTime = syncStartTime
	s.stats.mu.Unlock()

	log.Printf("[DepthSyncer] ğŸ¯ Starting market-separated depth sync")

	// è·å–ç°è´§å’ŒæœŸè´§äº¤æ˜“å¯¹é…ç½®
	syncerConfig := s.buildDepthSyncerConfig()

	totalUpdates := 0
	totalErrors := 0

	// åŒæ­¥ç°è´§å¸‚åœºæ·±åº¦
	if len(syncerConfig.SpotSymbols) > 0 {
		log.Printf("[DepthSyncer] ğŸ“ˆ Starting spot market depth sync for %d symbols", len(syncerConfig.SpotSymbols))
		spotUpdates, spotErrors := s.syncMarketDepth(ctx, syncerConfig.SpotSymbols, "spot")
		totalUpdates += spotUpdates
		totalErrors += spotErrors
	} else {
		log.Printf("[DepthSyncer] âš ï¸ No spot symbols to sync")
	}

	// åŒæ­¥æœŸè´§å¸‚åœºæ·±åº¦
	if len(syncerConfig.FuturesSymbols) > 0 {
		log.Printf("[DepthSyncer] ğŸ“ˆ Starting futures market depth sync for %d symbols", len(syncerConfig.FuturesSymbols))
		futuresUpdates, futuresErrors := s.syncMarketDepth(ctx, syncerConfig.FuturesSymbols, "futures")
		totalUpdates += futuresUpdates
		totalErrors += futuresErrors
	} else {
		log.Printf("[DepthSyncer] âš ï¸ No futures symbols to sync")
	}

	totalDuration := time.Since(syncStartTime)

	s.stats.mu.Lock()
	if totalErrors == 0 {
		s.stats.successfulSyncs++
	}
	s.stats.totalDepthUpdates += int64(totalUpdates)
	s.stats.mu.Unlock()

	// ç”Ÿæˆè¯¦ç»†çš„åŒæ­¥æŠ¥å‘Š
	log.Printf("[DepthSyncer] ğŸ“Š Depth sync completed in %v", totalDuration)
	log.Printf("[DepthSyncer] ğŸ“ˆ Total updates: %d", totalUpdates)
	log.Printf("[DepthSyncer] ğŸ“Š Markets synced: spot(%d), futures(%d)",
		len(syncerConfig.SpotSymbols), len(syncerConfig.FuturesSymbols))

	if totalErrors > 0 {
		log.Printf("[DepthSyncer] âš ï¸ %d markets had errors - check logs above", totalErrors)
		return fmt.Errorf("completed with %d market errors", totalErrors)
	}

	return nil
}

func (s *DepthSyncer) syncOrderBookDepth(ctx context.Context, symbol, kind string) error {
	// æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆç¬¦å·
	if s.redisCache != nil && s.redisCache.IsInvalid(symbol, kind) {
		return fmt.Errorf("symbol marked as invalid, skipping")
	}

	// ç­‰å¾…è·å–APIè°ƒç”¨ä»¤ç‰Œï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
	// ä½¿ç”¨æ·±åº¦ä¸“ç”¨é€Ÿç‡é™åˆ¶å™¨
	if err := DepthAPIRateLimiter.WaitForToken(ctx); err != nil {
		return fmt.Errorf("failed to acquire depth rate limit token: %w", err)
	}

	var url string
	if kind == "spot" {
		url = fmt.Sprintf("https://api.binance.com/api/v3/depth?symbol=%s&limit=20", symbol)
	} else {
		url = fmt.Sprintf("https://fapi.binance.com/fapi/v1/depth?symbol=%s&limit=20", symbol)
	}

	type OrderBook struct {
		LastUpdateId int64      `json:"lastUpdateId"`
		Bids         [][]string `json:"bids"` // [price, quantity]
		Asks         [][]string `json:"asks"` // [price, quantity]
	}

	var book OrderBook
	if err := netutil.GetJSON(ctx, url, &book); err != nil {
		// æ£€æŸ¥æ˜¯å¦ä¸ºæ— æ•ˆç¬¦å·é”™è¯¯
		errStr := err.Error()
		if strings.Contains(errStr, "Invalid symbol") || strings.Contains(errStr, "-1121") {
			// æ ‡è®°ä¸ºæ— æ•ˆç¬¦å·
			if s.redisCache != nil {
				if markErr := s.redisCache.MarkInvalid(symbol, kind); markErr != nil {
					log.Printf("[DepthSyncer] âš ï¸ Failed to mark invalid in Redis: %v", markErr)
				}
			}
			log.Printf("[DepthSyncer] ğŸ›‘ Marked %s %s as invalid symbol", symbol, kind)
			return fmt.Errorf("invalid symbol: %s %s", symbol, kind)
		}
		return fmt.Errorf("failed to get order book: %w", err)
	}

	// å°†æ•°æ®è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²
	bidsJSON, _ := json.Marshal(book.Bids)
	asksJSON, _ := json.Marshal(book.Asks)

	// åˆ›å»ºæ·±åº¦æ•°æ®å¯¹è±¡
	depthData := pdb.BinanceOrderBookDepth{
		Symbol:       symbol,
		MarketType:   kind,
		LastUpdateID: book.LastUpdateId,
		Bids:         string(bidsJSON),
		Asks:         string(asksJSON),
		SnapshotTime: time.Now().UnixMilli(), // ä½¿ç”¨æ¯«ç§’æ—¶é—´æˆ³
	}

	// ä¿å­˜åˆ°æ•°æ®åº“
	if err := pdb.SaveOrderBookDepth(s.db, []pdb.BinanceOrderBookDepth{depthData}); err != nil {
		return fmt.Errorf("failed to save order book depth: %w", err)
	}

	// è®¡ç®—ä¹°å–ä»·å·®ç”¨äºæ—¥å¿—
	if len(book.Bids) > 0 && len(book.Asks) > 0 {
		bestBid := book.Bids[0][0]
		bestAsk := book.Asks[0][0]
		spread := fmt.Sprintf("%.4f", (parseFloat(bestAsk)-parseFloat(bestBid))/parseFloat(bestBid)*100)

		log.Printf("[DepthSyncer] Saved %s %s depth - ID: %d, Bids: %d, Asks: %d, Spread: %s%%",
			symbol, kind, book.LastUpdateId, len(book.Bids), len(book.Asks), spread)
	}

	return nil
}

func (s *DepthSyncer) GetStats() map[string]interface{} {
	s.stats.mu.RLock()
	defer s.stats.mu.RUnlock()

	return map[string]interface{}{
		"total_syncs":      s.stats.totalSyncs,
		"successful_syncs": s.stats.successfulSyncs,
		"failed_syncs":     s.stats.failedSyncs,
		"last_sync_time":   s.stats.lastSyncTime,
		"total_updates":    s.stats.totalDepthUpdates,
	}
}

// GetAPIStats è·å–APIç»Ÿè®¡ä¿¡æ¯
func (s *DepthSyncer) GetAPIStats() *server.APIStats {
	s.stats.mu.RLock()
	defer s.stats.mu.RUnlock()

	successRate := "0%"
	if s.stats.totalAPICalls > 0 {
		rate := float64(s.stats.successfulAPICalls) / float64(s.stats.totalAPICalls) * 100
		successRate = fmt.Sprintf("%.1f%%", rate)
	}

	avgLatency := ""
	if s.stats.totalAPICalls > 0 && s.stats.totalLatency > 0 {
		avg := s.stats.totalLatency / time.Duration(s.stats.totalAPICalls)
		avgLatency = avg.String()
	}

	return &server.APIStats{
		TotalCalls:      s.stats.totalAPICalls,
		APICallsTotal:   s.stats.totalAPICalls,
		APISuccessRate:  successRate,
		APIAvgLatency:   &avgLatency,
		TotalSyncs:      s.stats.totalSyncs,
		SuccessfulSyncs: s.stats.successfulSyncs,
		FailedSyncs:     s.stats.failedSyncs,
		LastSyncTime:    &s.stats.lastSyncTime,
		TotalUpdates:    s.stats.totalDepthUpdates,
	}
}
