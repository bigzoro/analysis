package main

import (
	"bytes"
	"context"
	"encoding/json"
	"flag"
	"fmt"
	"log"
	"os"
	"os/signal"
	"strings"
	"sync"
	"syscall"
	"time"

	"analysis/internal/config"
	pdb "analysis/internal/db"

	"github.com/go-redis/redis/v8"
	"gopkg.in/yaml.v3"
	"gorm.io/gorm"
)

type DataSyncService struct {
	db     *gorm.DB
	server interface{} // æœåŠ¡å™¨å®ä¾‹ï¼Œç”¨äºè°ƒç”¨API
	cfg    *config.Config
	ctx    context.Context
	cancel context.CancelFunc

	// åŒæ­¥é…ç½®
	config DataSyncConfig

	// åŒæ­¥å™¨
	syncers map[string]DataSyncer

	// ç›‘æ§
	monitor *DataSyncMonitor

	// æ™ºèƒ½è°ƒåº¦å™¨
	smartScheduler *SmartScheduler

	// æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
	consistencyChecker *DataConsistencyChecker

	// ç›‘æ§ç³»ç»Ÿ
	monitoring *MonitoringSystem

	// Rediså®¢æˆ·ç«¯ - è·¨æœåŠ¡ç¼“å­˜
	redisClient *redis.Client

	// ç»Ÿè®¡æ›´æ–°å®šæ—¶å™¨
	statsUpdateTicker *time.Ticker
}

type DataSyncConfig struct {
	// åŒæ­¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰- æ”¯æŒå°æ•°ï¼Œå¦‚0.5è¡¨ç¤º30ç§’
	PriceSyncInterval        float64 `yaml:"price_sync_interval"`
	KlineSyncInterval        float64 `yaml:"kline_sync_interval"`
	FuturesSyncInterval      float64 `yaml:"futures_sync_interval"`
	EnableFundingHistory     bool    `yaml:"enable_funding_history"` // æ˜¯å¦å¯ç”¨å†å²èµ„é‡‘è´¹ç‡è·å–
	FundingHistoryHours      int     `yaml:"funding_history_hours"`  // å†å²èµ„é‡‘è´¹ç‡è·å–çš„æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
	DepthSyncInterval        float64 `yaml:"depth_sync_interval"`
	ExchangeInfoSyncInterval float64 `yaml:"exchange_info_sync_interval"`

	// åŒæ­¥å‚æ•°
	MaxRetries            int  `yaml:"max_retries"`
	RetryDelay            int  `yaml:"retry_delay"` // ç§’
	BatchSize             int  `yaml:"batch_size"`
	EnableHistoricalSync  bool `yaml:"enable_historical_sync"`
	EnableIncrementalSync bool `yaml:"enable_incremental_sync"` // æ˜¯å¦å¯ç”¨å¢é‡åŒæ­¥
	EnableRealtimeGainers bool `yaml:"enable_realtime_gainers"` // æ˜¯å¦å¯ç”¨å®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨

	// å®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨é…ç½®
	RealtimeGainers struct {
		Enabled         bool `yaml:"enabled"`
		TopSymbolsCount int  `yaml:"top_symbols_count"`
		UpdateInterval  int  `yaml:"update_interval"`

		// WebSocketè¿æ¥é…ç½®
		WebSocketReconnectDelay int `yaml:"websocket_reconnect_delay"`
		MaxWebSocketConnections int `yaml:"max_websocket_connections"`

		// ç¼“å­˜é…ç½®
		PriceCacheTTL            int `yaml:"price_cache_ttl"`
		BasePriceRefreshInterval int `yaml:"base_price_refresh_interval"`

		// å˜åŒ–æ£€æµ‹é˜ˆå€¼
		ChangeDetectThresholdRank   int     `yaml:"change_detect_threshold_rank"`
		ChangeDetectThresholdPrice  float64 `yaml:"change_detect_threshold_price"`
		ChangeDetectThresholdVolume float64 `yaml:"change_detect_threshold_volume"`

		// æ•°æ®åº“ä¿å­˜é…ç½®
		SaveBatchSize int `yaml:"save_batch_size"`
		SaveTimeout   int `yaml:"save_timeout"`

		// å¿«ç…§ç®¡ç†é…ç½®
		CleanupInterval        int `yaml:"cleanup_interval"`
		SnapshotRetentionHours int `yaml:"snapshot_retention_hours"`
		MaxSnapshotsPerKind    int `yaml:"max_snapshots_per_kind"`
	} `yaml:"realtime_gainers"`

	// åˆå§‹åŒ–æ¶¨å¹…æ¦œå¡«å……å™¨é…ç½®
	InitialGainersPopulator struct {
		Enabled            bool `yaml:"enabled"`
		PopulateOnStartup  bool `yaml:"populate_on_startup"`
		PopulateThreshold  int  `yaml:"populate_threshold"`
		PopulateLimit      int  `yaml:"populate_limit"`
		DataRetentionHours int  `yaml:"data_retention_hours"`
		CleanupInterval    int  `yaml:"cleanup_interval"`
	} `yaml:"initial_gainers_populator"`

	// æ•°æ®æºé…ç½®
	Exchanges      []string `yaml:"exchanges"`
	Symbols        []string `yaml:"symbols"`
	KlineIntervals []string `yaml:"kline_intervals"`

	// ç›‘æ§é…ç½®
	EnableMetrics   bool `yaml:"enable_metrics"`
	MetricsInterval int  `yaml:"metrics_interval"` // åˆ†é’Ÿ

	// æ•°æ®è´¨é‡æ£€æŸ¥
	EnableDataValidation bool `yaml:"enable_data_validation"`
	MaxDataAgeMinutes    int  `yaml:"max_data_age_minutes"`

	// å­˜å‚¨é…ç½®
	EnableCompression bool `yaml:"enable_compression"`
	RetentionDays     int  `yaml:"retention_days"`

	// ç½‘ç»œé…ç½®
	TimeoutSeconds    int `yaml:"timeout_seconds"`
	RateLimitRequests int `yaml:"rate_limit_requests"`
	RateLimitBurst    int `yaml:"rate_limit_burst"`

	// å¹¶å‘æ§åˆ¶ - ä¼˜åŒ–å‚æ•°
	WorkerPoolSize       int `yaml:"worker_pool_size"`
	MaxConcurrentSymbols int `yaml:"max_concurrent_symbols"`
	APICallTimeout       int `yaml:"api_call_timeout"`

	// ç¼“å­˜é…ç½® - ä¼˜åŒ–å‚æ•°
	EnableCaching   bool `yaml:"enable_caching"`
	CacheTTLSeconds int  `yaml:"cache_ttl_seconds"`
	CacheMaxSize    int  `yaml:"cache_max_size"`

	// Redisé…ç½® - è·¨æœåŠ¡ç¼“å­˜
	EnableRedisCache bool   `yaml:"enable_redis_cache"`
	RedisAddr        string `yaml:"redis_addr"`
	RedisPassword    string `yaml:"redis_password"`
	RedisDB          int    `yaml:"redis_db"`
	RedisKeyPrefix   string `yaml:"redis_key_prefix"`

	// WebSocketé…ç½® - é«˜é¢‘æ•°æ®åŒæ­¥
	EnableWebSocketSync          bool `yaml:"enable_websocket_sync"`
	WebSocketBatchInterval       int  `yaml:"websocket_batch_interval"`
	WebSocketMaxSymbols          int  `yaml:"websocket_max_symbols"`
	WebSocketReconnectDelay      int  `yaml:"websocket_reconnect_delay"`
	WebSocketHealthCheckInterval int  `yaml:"websocket_health_check_interval"`
	WebSocketEnableAutoAdjust    bool `yaml:"websocket_enable_auto_adjust"`

	// æ™ºèƒ½è°ƒåº¦å™¨é…ç½®
	SmartScheduler struct {
		Enabled              bool    `yaml:"enabled"`
		CheckInterval        int     `yaml:"check_interval"`
		WebSocketGracePeriod int     `yaml:"websocket_grace_period"`
		RestAPIBackoffFactor float64 `yaml:"rest_api_backoff_factor"`
	} `yaml:"smart_scheduler"`

	// æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨é…ç½®
	DataConsistency struct {
		Enabled           bool `yaml:"enabled"`
		CheckInterval     int  `yaml:"check_interval"`
		ConsistencyWindow int  `yaml:"consistency_window"`
		MaxDataAge        int  `yaml:"max_data_age"`
	} `yaml:"data_consistency"`

	// ç›‘æ§ç³»ç»Ÿé…ç½®
	Monitoring struct {
		Enabled       bool `yaml:"enabled"`
		CheckInterval int  `yaml:"check_interval"`
		AlertCooldown int  `yaml:"alert_cooldown"`
		Thresholds    struct {
			WebSocketReconnectThreshold int     `yaml:"websocket_reconnect_threshold"`
			WebSocketDowntimeThreshold  int     `yaml:"websocket_downtime_threshold"`
			APIFailureRateThreshold     float64 `yaml:"api_failure_rate_threshold"`
			APILatencyThreshold         int     `yaml:"api_latency_threshold"`
			DataConsistencyThreshold    float64 `yaml:"data_consistency_threshold"`
			DataAgeThreshold            int     `yaml:"data_age_threshold"`
			MemoryUsageThreshold        float64 `yaml:"memory_usage_threshold"`
			CPUUsageThreshold           float64 `yaml:"cpu_usage_threshold"`
			GoroutineCountThreshold     int     `yaml:"goroutine_count_threshold"`
		} `yaml:"thresholds"`
	} `yaml:"monitoring"`

	// è¶…æ—¶å’Œæ—¶é—´å¸¸é‡é…ç½®
	Timeouts struct {
		APICallTimeout              int `yaml:"api_call_timeout"`
		WebSocketReadTimeout        int `yaml:"websocket_read_timeout"`
		WebSocketHealthCheckTimeout int `yaml:"websocket_health_check_timeout"`
		WebSocketReconnectDelay     int `yaml:"websocket_reconnect_delay"`
		DataAgeMax                  int `yaml:"data_age_max"`
		ConsistencyCheckInterval    int `yaml:"consistency_check_interval"`
	} `yaml:"timeouts"`
}

type DataSyncMonitor struct {
	mu        sync.RWMutex
	stats     map[string]map[string]interface{}
	startTime time.Time
}

func NewDataSyncService(db *gorm.DB, server interface{}, cfg *config.Config) *DataSyncService {
	ctx, cancel := context.WithCancel(context.Background())

	service := &DataSyncService{
		db:      db,
		server:  server,
		cfg:     cfg,
		ctx:     ctx,
		cancel:  cancel,
		config:  DataSyncConfig{}, // ä½¿ç”¨é›¶å€¼ï¼Œä¾èµ–é…ç½®æ–‡ä»¶æä¾›æ‰€æœ‰é…ç½®
		syncers: make(map[string]DataSyncer),
		monitor: &DataSyncMonitor{
			stats:     make(map[string]map[string]interface{}),
			startTime: time.Now(),
		},
	}

	// å¦‚æœæ•°æ®åº“ä¸ºnilï¼Œè·³è¿‡åˆå§‹åŒ–ï¼ˆå°†åœ¨åç»­è®¾ç½®æ•°æ®åº“åé‡æ–°åˆå§‹åŒ–ï¼‰
	if db != nil {
		// åˆå§‹åŒ–Rediså®¢æˆ·ç«¯
		service.initRedisClient()

		// åˆå§‹åŒ–åŒæ­¥å™¨
		service.initSyncers()
	}

	return service
}

// initRedisClient åˆå§‹åŒ–Rediså®¢æˆ·ç«¯
func (s *DataSyncService) initRedisClient() {
	if !s.config.EnableRedisCache {
		log.Println("[DataSync] Redis cache disabled, using in-memory cache only")
		return
	}

	// åˆ›å»ºRediså®¢æˆ·ç«¯
	rdb := redis.NewClient(&redis.Options{
		Addr:     s.config.RedisAddr,
		Password: s.config.RedisPassword,
		DB:       s.config.RedisDB,
	})

	// æµ‹è¯•è¿æ¥
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	if _, err := rdb.Ping(ctx).Result(); err != nil {
		log.Printf("[DataSync] âš ï¸ Failed to connect to Redis: %v, falling back to in-memory cache", err)
		return
	}

	s.redisClient = rdb
	log.Printf("[DataSync] âœ… Connected to Redis at %s (DB: %d)", s.config.RedisAddr, s.config.RedisDB)
}

// registerConditionalSyncers æ³¨å†Œéœ€è¦æ ¹æ®é…ç½®æ¡ä»¶å†³å®šçš„åŒæ­¥å™¨
func (s *DataSyncService) registerConditionalSyncers() {
	log.Printf("[DataSync] ===== å®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨æ³¨å†Œæ£€æŸ¥ =====")
	log.Printf("[DataSync] æ£€æŸ¥å®æ—¶æ¶¨å¹…æ¦œé…ç½®: EnableRealtimeGainers=%v", s.config.EnableRealtimeGainers)
	log.Printf("[DataSync] æ•°æ®åº“è¿æ¥çŠ¶æ€: %v", s.db != nil)
	log.Printf("[DataSync] é…ç½®æ–‡ä»¶çŠ¶æ€: %v", s.cfg != nil)

	if s.config.EnableRealtimeGainers {
		log.Printf("[DataSync] âœ… é…ç½®å¯ç”¨ï¼Œå¼€å§‹åˆ›å»ºå®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨...")

		// åˆ›å»ºç°è´§å¸‚åœºæ¶¨å¹…æ¦œåŒæ­¥å™¨
		realtimeGainersSyncerSpot := NewRealtimeGainersSyncerWithKind(s.db, s.cfg, &s.config, "spot")
		if realtimeGainersSyncerSpot != nil {
			s.syncers["realtime_gainers_spot"] = realtimeGainersSyncerSpot
			log.Printf("[DataSync] âœ… ç°è´§å¸‚åœºå®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨åˆ›å»ºæˆåŠŸ")
		} else {
			log.Printf("[DataSync] âŒ ç°è´§å¸‚åœºå®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨åˆ›å»ºå¤±è´¥")
		}

		// åˆ›å»ºæœŸè´§å¸‚åœºæ¶¨å¹…æ¦œåŒæ­¥å™¨
		realtimeGainersSyncerFutures := NewRealtimeGainersSyncerWithKind(s.db, s.cfg, &s.config, "futures")
		if realtimeGainersSyncerFutures != nil {
			s.syncers["realtime_gainers_futures"] = realtimeGainersSyncerFutures
			log.Printf("[DataSync] âœ… æœŸè´§å¸‚åœºå®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨åˆ›å»ºæˆåŠŸ")
		} else {
			log.Printf("[DataSync] âŒ æœŸè´§å¸‚åœºå®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨åˆ›å»ºå¤±è´¥")
		}

		log.Printf("[DataSync] å½“å‰æ³¨å†Œçš„åŒæ­¥å™¨æ•°é‡: %d", len(s.syncers))
		log.Printf("[DataSync] å·²æ³¨å†Œçš„åŒæ­¥å™¨: %v", getSyncerNames(s.syncers))
	} else {
		log.Printf("[DataSync] âŒ å®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨æœªå¯ç”¨ (é…ç½®è¢«ç¦ç”¨)")
	}
	log.Printf("[DataSync] ===== å®æ—¶æ¶¨å¹…æ¦œåŒæ­¥å™¨æ³¨å†Œæ£€æŸ¥ç»“æŸ =====")
}

func (s *DataSyncService) initSyncers() {
	// åˆ›å»ºRedisç¼“å­˜å®ä¾‹
	redisCache := NewRedisInvalidSymbolCache(s.redisClient, s.config.RedisKeyPrefix, time.Hour*24)

	// ä»·æ ¼åŒæ­¥å™¨
	priceSyncer := NewPriceSyncer(s.db, s.cfg, &s.config, redisCache)
	s.syncers["price"] = priceSyncer

	// Kçº¿åŒæ­¥å™¨
	//s.syncers["kline"] = NewKlineSyncer(s.db, s.server, s.cfg, &s.config, redisCache)

	// æœŸè´§ä¿¡æ¯åŒæ­¥å™¨
	s.syncers["futures"] = NewFuturesSyncer(s.db, s.cfg, &s.config)

	// æ·±åº¦åŒæ­¥å™¨
	//s.syncers["depth"] = NewDepthSyncer(s.db, s.cfg, &s.config, redisCache)

	// å¸‚åœºç»Ÿè®¡æ•°æ®åŒæ­¥å™¨ - åŒæ­¥24å°æ—¶å¸‚åœºç»Ÿè®¡æ•°æ®ï¼ŒåŒ…æ‹¬ä»·æ ¼ã€äº¤æ˜“é‡ã€ä¹°å–ç›˜å£ç­‰å®Œæ•´ä¿¡æ¯
	s.syncers["market_stats"] = NewMarketStatsSyncer(s.db, s.cfg, &s.config, redisCache)

	// äº¤æ˜“å¯¹ä¿¡æ¯åŒæ­¥å™¨
	s.syncers["exchange_info"] = NewExchangeInfoSyncer(s.db, s.cfg, &s.config)

	// æ¶¨å¹…æ¦œåˆå§‹åŒ–æ•°æ®å¡«å……å™¨ - ç³»ç»Ÿå¯åŠ¨æ—¶æä¾›åˆå§‹æ¶¨å¹…æ¦œæ•°æ®
	initialGainersPopulator := NewInitialGainersPopulator(s.db, s.cfg, &s.config)
	s.syncers["initial_gainers"] = initialGainersPopulator

	// WebSocketåŒæ­¥å™¨ï¼ˆå®éªŒæ€§ï¼‰
	if s.config.EnableWebSocketSync {
		websocketSyncer := NewWebSocketSyncer(s.db, &s.config)
		s.syncers["websocket"] = websocketSyncer

		// è®¾ç½®ä»·æ ¼åŒæ­¥å™¨çš„WebSocketå¼•ç”¨
		priceSyncer.SetWebSocketSyncer(websocketSyncer)

		// åˆå§‹åŒ–æ™ºèƒ½è°ƒåº¦å™¨
		if s.config.SmartScheduler.Enabled {
			// æ£€æŸ¥å¿…è¦çš„åŒæ­¥å™¨æ˜¯å¦å­˜åœ¨
			klineSyncer, hasKline := s.syncers["kline"]
			if !hasKline {
				log.Printf("[DataSync] âš ï¸  Kline syncer not available, skipping smart scheduler initialization")
			} else {
				s.smartScheduler = NewSmartSchedulerWithConfig(
					websocketSyncer,
					klineSyncer.(*KlineSyncer),
					s.syncers["depth"].(*DepthSyncer),
					priceSyncer,
					&s.config,
				)
				log.Printf("[DataSync] Smart scheduler initialized with config")
			}
		}

		// åˆå§‹åŒ–æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
		if s.config.DataConsistency.Enabled {
			// æ£€æŸ¥å¿…è¦çš„åŒæ­¥å™¨æ˜¯å¦å­˜åœ¨
			klineSyncer, hasKline := s.syncers["kline"]
			if !hasKline {
				log.Printf("[DataSync] âš ï¸  Kline syncer not available, skipping data consistency checker initialization")
			} else {
				s.consistencyChecker = NewDataConsistencyCheckerWithConfig(
					s.db,
					websocketSyncer,
					klineSyncer.(*KlineSyncer),
					s.syncers["depth"].(*DepthSyncer),
					priceSyncer,
					&s.config,
				)
				log.Printf("[DataSync] Data consistency checker initialized with config")
			}
		}

		// åˆå§‹åŒ–ç›‘æ§ç³»ç»Ÿ
		if s.config.Monitoring.Enabled {
			s.monitoring = NewMonitoringSystem(s)
			log.Printf("[DataSync] Monitoring system initialized")
		}
	}
}

func (s *DataSyncService) Start(initialSyncMode string) error {
	log.Printf("[DataSync] Starting data synchronization service...")

	// åœ¨æ¸…ç†ç¼“å­˜ä¹‹å‰ï¼Œå…ˆåŒæ­¥äº¤æ˜“å¯¹ä¿¡æ¯ï¼Œç¡®ä¿æ•°æ®åº“æ•°æ®æœ€æ–°
	if exchangeInfoSyncer, exists := s.syncers["exchange_info"]; exists {
		log.Printf("[DataSync] ğŸ“‹ Pre-syncing exchange info before cache cleanup...")

		// åˆ›å»ºå¸¦è¶…æ—¶çš„ä¸Šä¸‹æ–‡ï¼Œé¿å…é˜»å¡å¤ªä¹…ï¼ˆæœ€å¤š30ç§’ï¼‰
		syncCtx, cancel := context.WithTimeout(s.ctx, 30*time.Second)
		defer cancel()

		if err := exchangeInfoSyncer.Sync(syncCtx); err != nil {
			log.Printf("[DataSync] âš ï¸ Failed to pre-sync exchange info: %v", err)
			// ä¸å› ä¸ºè¿™ä¸ªé”™è¯¯è€Œåœæ­¢å¯åŠ¨ï¼Œç»§ç»­æ‰§è¡Œ
		} else {
			log.Printf("[DataSync] âœ… Exchange info pre-sync completed")
		}
	} else {
		log.Printf("[DataSync] âš ï¸ Exchange info syncer not found, skipping pre-sync")
	}

	// æ¸…ç†Redisç¼“å­˜ä¸­çš„è¿‡æœŸæ— æ•ˆç¬¦å·
	if s.redisClient != nil {
		redisCache := NewRedisInvalidSymbolCache(s.redisClient, s.config.RedisKeyPrefix, time.Hour*24)
		if err := redisCache.CleanupInvalidSymbols(s.db); err != nil {
			log.Printf("[DataSync] âš ï¸ Failed to cleanup invalid symbols cache: %v", err)
		} else {
			log.Printf("[DataSync] âœ… Invalid symbols cache cleanup completed")
		}
	}

	// å¦‚æœé…ç½®ä¸­æ²¡æœ‰æŒ‡å®šäº¤æ˜“å¯¹ï¼Œåˆ™ä»æ•°æ®åº“åŠ¨æ€è·å–
	// æ³¨æ„ï¼šè¿™é‡Œè·å–çš„æ˜¯æ‰€æœ‰äº¤æ˜“å¯¹ï¼Œä½†å„ä¸ªåŒæ­¥å™¨ä¼šæ ¹æ®è‡ªèº«éœ€æ±‚è¿‡æ»¤
	if len(s.config.Symbols) == 0 {
		log.Printf("[DataSync] No symbols configured, fetching from database...")
		symbols, err := pdb.GetUSDTTradingPairs(s.db)
		if err != nil {
			log.Printf("[DataSync] Failed to fetch symbols from database: %v", err)
			return fmt.Errorf("failed to fetch symbols from database: %w", err)
		}

		if len(symbols) == 0 {
			log.Printf("[DataSync] No symbols found in database, using default fallback symbols...")

			// ä½¿ç”¨æ ¸å¿ƒäº¤æ˜“å¯¹ä½œä¸ºfallbackï¼Œé¿å…ç©ºåˆ—è¡¨å¯¼è‡´åŒæ­¥å¤±è´¥
			coreSymbols := []string{"BTCUSDT", "ETHUSDT", "BNBUSDT"}
			symbols = coreSymbols

			log.Printf("[DataSync] Using %d core symbols as fallback: %v", len(symbols), symbols)
		}

		s.config.Symbols = symbols
		log.Printf("[DataSync] Dynamically loaded %d symbols from database", len(symbols))
	}

	log.Printf("[DataSync] Configuration: Price=%.0fm, Kline=%.0fm, Futures=%.0fm, Depth=%.0fm",
		s.config.PriceSyncInterval, s.config.KlineSyncInterval,
		s.config.FuturesSyncInterval, s.config.DepthSyncInterval)
	log.Printf("[DataSync] Symbols to sync: %d symbols", len(s.config.Symbols))
	log.Printf("[DataSync] Exchanges: %v", s.config.Exchanges)

	// æ ¹æ®åˆå§‹åŒæ­¥æ¨¡å¼å†³å®šå¦‚ä½•æ‰§è¡Œåˆå§‹åŒæ­¥æµ‹è¯•
	switch initialSyncMode {
	case "skip":
		log.Printf("[DataSync] Skipping initial sync test as requested")
	case "ordered":
		log.Printf("[DataSync] Running initial sync test in ordered mode...")

		// å®šä¹‰é¦–æ¬¡åŒæ­¥çš„æ‰§è¡Œé¡ºåºï¼šå…ˆåŒæ­¥äº¤æ˜“å¯¹ä¿¡æ¯ï¼Œå†åŒæ­¥å¸‚åœºæ•°æ®ï¼Œæœ€ååŒæ­¥æ¶¨å¹…æ¦œç›¸å…³æ•°æ®
		orderedSyncers := []string{"exchange_info", "market_stats", "initial_gainers", "realtime_gainers"}
		executedSyncers := make(map[string]bool)

		log.Printf("[DataSync] Ordered syncers to test: %v", orderedSyncers)
		log.Printf("[DataSync] Available syncers: %v", getSyncerNames(s.syncers))

		// æŒ‰æŒ‡å®šé¡ºåºæ‰§è¡Œå…³é”®åŒæ­¥å™¨
		for _, syncerName := range orderedSyncers {
			if syncerName == "realtime_gainers" {
				// ç‰¹æ®Šå¤„ç†å®æ—¶æ¶¨å¹…æ¦œï¼šå®ƒè¢«æ³¨å†Œä¸ºä¸¤ä¸ªåŒæ­¥å™¨ (spot å’Œ futures)
				realtimeSyncers := []string{"realtime_gainers_spot", "realtime_gainers_futures"}
				allRealtimePassed := true

				for _, rtSyncerName := range realtimeSyncers {
					if syncer, exists := s.syncers[rtSyncerName]; exists {
						log.Printf("[DataSync] Testing syncer: %s (part of %s)", rtSyncerName, syncerName)
						if err := syncer.Sync(s.ctx); err != nil {
							log.Printf("[DataSync] âŒ Initial sync test failed for %s: %v", rtSyncerName, err)
							allRealtimePassed = false
						} else {
							log.Printf("[DataSync] âœ… Initial sync test passed for %s", rtSyncerName)
						}
						executedSyncers[rtSyncerName] = true
					} else {
						log.Printf("[DataSync] âš ï¸  Realtime gainers syncer %s not found, skipping", rtSyncerName)
					}
				}

				if allRealtimePassed {
					log.Printf("[DataSync] âœ… Initial sync test passed for realtime_gainers")
				} else {
					log.Printf("[DataSync] âŒ Initial sync test failed for realtime_gainers")
				}
				executedSyncers[syncerName] = true
			} else if syncer, exists := s.syncers[syncerName]; exists {
				log.Printf("[DataSync] Testing syncer: %s (ordered)", syncerName)
				if err := syncer.Sync(s.ctx); err != nil {
					log.Printf("[DataSync] âŒ Initial sync test failed for %s: %v", syncerName, err)
				} else {
					log.Printf("[DataSync] âœ… Initial sync test passed for %s", syncerName)
				}
				executedSyncers[syncerName] = true
			} else {
				log.Printf("[DataSync] âš ï¸  Syncer %s not found in syncers map, skipping", syncerName)
			}
		}

		// æ‰§è¡Œå‰©ä½™çš„åŒæ­¥å™¨ï¼ˆè·³è¿‡å·²æ‰§è¡Œçš„ï¼‰
		for name, syncer := range s.syncers {
			if executedSyncers[name] {
				continue
			}
			log.Printf("[DataSync] Testing syncer: %s", name)
			if err := syncer.Sync(s.ctx); err != nil {
				log.Printf("[DataSync] âŒ Initial sync test failed for %s: %v", name, err)
			} else {
				log.Printf("[DataSync] âœ… Initial sync test passed for %s", name)
			}
		}
	case "random":
		log.Printf("[DataSync] Running initial sync test in random mode...")

		// éšæœºé¡ºåºæ‰§è¡Œæ‰€æœ‰åŒæ­¥å™¨
		for name, syncer := range s.syncers {
			log.Printf("[DataSync] Testing syncer: %s", name)
			if err := syncer.Sync(s.ctx); err != nil {
				log.Printf("[DataSync] âŒ Initial sync test failed for %s: %v", name, err)
			} else {
				log.Printf("[DataSync] âœ… Initial sync test passed for %s", name)
			}
		}
	default:
		log.Printf("[DataSync] Unknown initial sync mode '%s', defaulting to 'ordered'", initialSyncMode)
		// é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨é»˜è®¤çš„ ordered æ¨¡å¼
		return s.Start("ordered")
	}

	// å¯åŠ¨æ‰€æœ‰åŒæ­¥å™¨
	for name, syncer := range s.syncers {
		log.Printf("[DataSync] Starting syncer: %s", name)

		var interval time.Duration
		switch name {
		case "price":
			interval = time.Duration(s.config.PriceSyncInterval*60) * time.Second
		case "kline":
			interval = time.Duration(s.config.KlineSyncInterval*60) * time.Second
		case "futures":
			interval = time.Duration(s.config.FuturesSyncInterval*60) * time.Second
		case "depth":
			interval = time.Duration(s.config.DepthSyncInterval*60) * time.Second
		case "market_stats":
			interval = time.Duration(s.config.KlineSyncInterval*60) * time.Second
		case "exchange_info":
			interval = time.Duration(s.config.ExchangeInfoSyncInterval*60) * time.Second
		case "initial_gainers":
			// åˆå§‹åŒ–å¡«å……å™¨åªåœ¨å¯åŠ¨æ—¶è¿è¡Œä¸€æ¬¡ï¼Œä¸éœ€è¦å®šæœŸè¿è¡Œ
			log.Printf("[DataSync] Starting initial gainers populator...")
			go syncer.Start(s.ctx, 0) // ä¼ é€’0é—´éš”ï¼Œè¡¨ç¤ºä¸€æ¬¡æ€§è¿è¡Œ
			continue
		default:
			interval = 5 * time.Minute
		}

		log.Printf("[DataSync] %s syncer will run every %v", name, interval)
		go syncer.Start(s.ctx, interval)
	}

	// å¯åŠ¨æ™ºèƒ½è°ƒåº¦å™¨
	if s.smartScheduler != nil {
		log.Printf("[DataSync] Starting smart scheduler for intelligent WebSocket/REST API coordination")
		s.smartScheduler.Start()
	}

	// å¯åŠ¨æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
	if s.consistencyChecker != nil {
		log.Printf("[DataSync] Starting data consistency checker")
		s.consistencyChecker.Start()
	}

	// å¯åŠ¨ç›‘æ§ç³»ç»Ÿ
	if s.monitoring != nil {
		log.Printf("[DataSync] Starting monitoring system")
		s.monitoring.Start()
	}

	// WebSocketçŠ¶æ€æ£€æŸ¥
	if websocketSyncer, exists := s.syncers["websocket"]; exists {
		go func() {
			// ç­‰å¾…10ç§’è®©WebSocketå»ºç«‹è¿æ¥
			time.Sleep(10 * time.Second)

			if ws, ok := websocketSyncer.(*WebSocketSyncer); ok {
				healthStatus := ws.GetHealthStatus()
				log.Printf("[DataSync] ğŸ“Š WebSocket startup status check:")
				log.Printf("[DataSync]   - Running: %v", healthStatus["is_running"])
				log.Printf("[DataSync]   - Healthy: %v", healthStatus["is_healthy"])
				log.Printf("[DataSync]   - Spot connections: %v/%v healthy",
					healthStatus["healthy_spot"], healthStatus["spot_connections"])
				log.Printf("[DataSync]   - Futures connections: %v/%v healthy",
					healthStatus["healthy_futures"], healthStatus["futures_connections"])
				log.Printf("[DataSync]   - Messages received: %v", healthStatus["messages_received"])
				log.Printf("[DataSync]   - Last message: %v", healthStatus["time_since_last_message"])
			}
		}()
	}

	// å¯åŠ¨ç›‘æ§
	if s.config.EnableMetrics {
		log.Printf("[DataSync] Starting metrics reporter (every %d minutes)", s.config.MetricsInterval)
		go s.startMetricsReporter()
	}

	// å¯åŠ¨å¿ƒè·³æ—¥å¿—
	go s.startHeartbeat()

	// å¯åŠ¨å¥åº·æ£€æŸ¥
	go s.startHealthCheck()

	// å¯åŠ¨ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å™¨
	log.Printf("[DataSync] Starting stats updater")
	s.startStatsUpdater()

	log.Printf("[DataSync] Data synchronization service started successfully")
	log.Printf("[DataSync] Service will continue running. Press Ctrl+C to stop.")
	log.Printf("[DataSync] ğŸ’¡ Tips:")
	log.Printf("[DataSync]   - Use 'test-sync' to validate all syncers")
	log.Printf("[DataSync]   - Use 'sync-once kline' to test kline sync")
	log.Printf("[DataSync]   - Check logs for detailed performance metrics")

	return nil
}

func (s *DataSyncService) Stop() {
	log.Printf("[DataSync] Stopping data synchronization service...")

	s.cancel()

	// åœæ­¢æ™ºèƒ½è°ƒåº¦å™¨
	if s.smartScheduler != nil {
		log.Printf("[DataSync] Stopping smart scheduler")
		s.smartScheduler.Stop()
	}

	// åœæ­¢æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥å™¨
	if s.consistencyChecker != nil {
		log.Printf("[DataSync] Stopping data consistency checker")
		s.consistencyChecker.Stop()
	}

	// åœæ­¢ç›‘æ§ç³»ç»Ÿ
	if s.monitoring != nil {
		log.Printf("[DataSync] Stopping monitoring system")
		s.monitoring.Stop()
	}

	// åœæ­¢æ‰€æœ‰åŒæ­¥å™¨
	for name, syncer := range s.syncers {
		log.Printf("[DataSync] Stopping syncer: %s", name)
		syncer.Stop()
	}

	log.Printf("[DataSync] Data synchronization service stopped")
}

func (s *DataSyncService) startHeartbeat() {
	ticker := time.NewTicker(30 * time.Second) // æ¯30ç§’å¿ƒè·³ä¸€æ¬¡
	defer ticker.Stop()

	heartbeatCount := 0

	for {
		select {
		case <-s.ctx.Done():
			log.Printf("[DataSync] Heartbeat stopped")
			return
		case <-ticker.C:
			heartbeatCount++
			uptime := time.Since(s.monitor.startTime)

			// æ£€æŸ¥æ•°æ®åº“è¿æ¥
			dbHealthy := s.checkDatabaseHealth()

			status := "âœ…"
			if !dbHealthy {
				status = "âŒ"
			}

			log.Printf("[DataSync] %s Heartbeat #%d - Uptime: %v - DB: %s",
				status, heartbeatCount, formatDuration(uptime),
				map[bool]string{true: "healthy", false: "unhealthy"}[dbHealthy])
		}
	}
}

func (s *DataSyncService) startHealthCheck() {
	ticker := time.NewTicker(5 * time.Minute) // æ¯5åˆ†é’Ÿè¿›è¡Œä¸€æ¬¡å¥åº·æ£€æŸ¥
	defer ticker.Stop()

	for {
		select {
		case <-s.ctx.Done():
			return
		case <-ticker.C:
			s.performHealthCheck()
		}
	}
}

func (s *DataSyncService) performHealthCheck() {
	log.Printf("[DataSync] ğŸ” Performing health check...")

	issues := 0

	// æ£€æŸ¥æ•°æ®åº“è¿æ¥
	if !s.checkDatabaseHealth() {
		log.Printf("[DataSync] âŒ Database connection unhealthy")
		issues++
	} else {
		log.Printf("[DataSync] âœ… Database connection healthy")
	}

	// æ£€æŸ¥åŒæ­¥å™¨çŠ¶æ€
	for name, syncer := range s.syncers {
		stats := syncer.GetStats()
		lastSync, ok := stats["last_sync_time"]
		if !ok {
			log.Printf("[DataSync] âš ï¸ %s syncer has no sync history", name)
			issues++
			continue
		}

		// æ£€æŸ¥æœ€ååŒæ­¥æ—¶é—´
		if lastSyncTime, ok := lastSync.(time.Time); ok {
			timeSinceLastSync := time.Since(lastSyncTime)
			if timeSinceLastSync > 10*time.Minute {
				log.Printf("[DataSync] âš ï¸ %s syncer last synced %v ago", name, timeSinceLastSync)
				issues++
			} else {
				log.Printf("[DataSync] âœ… %s syncer healthy (last sync: %v ago)", name, timeSinceLastSync)
			}
		}
	}

	if issues == 0 {
		log.Printf("[DataSync] ğŸ‰ Health check passed - all systems operational")
	} else {
		log.Printf("[DataSync] âš ï¸ Health check found %d issues - check logs above", issues)
	}
}

func (s *DataSyncService) checkDatabaseHealth() bool {
	// ç®€å•çš„æ•°æ®åº“å¥åº·æ£€æŸ¥
	db, err := s.db.DB()
	if err != nil {
		return false
	}

	// å°è¯•æ‰§è¡Œä¸€ä¸ªç®€å•çš„æŸ¥è¯¢
	var result int
	row := db.QueryRow("SELECT 1")
	err = row.Scan(&result)
	return err == nil && result == 1
}

func (s *DataSyncService) startMetricsReporter() {
	ticker := time.NewTicker(time.Duration(s.config.MetricsInterval) * time.Minute)
	defer ticker.Stop()

	for {
		select {
		case <-s.ctx.Done():
			return
		case <-ticker.C:
			s.reportMetrics()
		}
	}
}

func (s *DataSyncService) reportMetrics() {
	log.Printf("[DataSync] === Data Sync Metrics Report ===")

	s.monitor.mu.RLock()
	defer s.monitor.mu.RUnlock()

	totalUptime := time.Since(s.monitor.startTime)

	for name, syncer := range s.syncers {
		stats := syncer.GetStats()
		log.Printf("[DataSync] %s Syncer Stats:", strings.Title(name))
		for key, value := range stats {
			log.Printf("[DataSync]   %s: %v", key, value)
		}
	}

	log.Printf("[DataSync] Total Uptime: %v", totalUptime)
	log.Printf("[DataSync] === End Metrics Report ===")
}

func (s *DataSyncService) SyncOnce(syncerName string) error {
	if syncer, exists := s.syncers[syncerName]; exists {
		log.Printf("[DataSync] Running one-time sync for: %s", syncerName)
		return syncer.Sync(s.ctx)
	}
	return fmt.Errorf("syncer not found: %s", syncerName)
}

func (s *DataSyncService) GetStatus() map[string]interface{} {
	s.monitor.mu.RLock()
	defer s.monitor.mu.RUnlock()

	status := map[string]interface{}{
		"service":    "data_sync",
		"start_time": s.monitor.startTime,
		"uptime":     time.Since(s.monitor.startTime).String(),
		"syncers":    make(map[string]interface{}),
	}

	for name, syncer := range s.syncers {
		status["syncers"].(map[string]interface{})[name] = syncer.GetStats()
	}

	return status
}

func main() {
	// å‘½ä»¤è¡Œå‚æ•°
	action := flag.String("action", "start", "æ“ä½œç±»å‹: start(å¯åŠ¨æœåŠ¡), test-sync(æµ‹è¯•æ‰€æœ‰åŒæ­¥å™¨), sync-once(å•æ¬¡åŒæ­¥), status(çŠ¶æ€æŸ¥è¯¢)")
	syncerName := flag.String("syncer", "", "åŒæ­¥å™¨åç§° (ç”¨äºsync-onceæ“ä½œ)")
	configPath := flag.String("config", "./config.yaml", "é…ç½®æ–‡ä»¶è·¯å¾„")
	initialSyncMode := flag.String("initial-sync-mode", "ordered", "åˆå§‹åŒæ­¥æ¨¡å¼: skip(è·³è¿‡), ordered(é¡ºåºæ‰§è¡Œ), random(éšæœºæ‰§è¡Œ)")

	flag.Parse()

	fmt.Printf("[data_sync] Starting data synchronizati on service, action=%s\n", *action)

	// ä¸€æ¬¡æ€§è¯»å–å¹¶è§£æé…ç½®æ–‡ä»¶
	fmt.Printf("[data_sync] Attempting to load config from: %s\n", *configPath)

	// è·å–å½“å‰å·¥ä½œç›®å½•
	if cwd, err := os.Getwd(); err == nil {
		fmt.Printf("[data_sync] Current working directory: %s\n", cwd)
	}

	configData, err := os.ReadFile(*configPath)
	if err != nil {
		fmt.Printf("[data_sync] Failed to read config file %s: %v\n", *configPath, err)
		return
	}
	fmt.Printf("[data_sync] Successfully read config file: %s (%d bytes)\n", *configPath, len(configData))

	// ä¸€æ¬¡æ€§è§£ææ•´ä¸ªé…ç½®æ–‡ä»¶
	var fullConfig map[string]interface{}
	if err := yaml.Unmarshal(configData, &fullConfig); err != nil {
		fmt.Printf("[data_sync] Failed to parse config file: %v\n", err)
		return
	}

	// æ‰“å°æ‰€æœ‰é¡¶çº§é…ç½®é¡¹
	fmt.Printf("[data_sync] Found top-level config sections:\n")
	for key := range fullConfig {
		fmt.Printf("[data_sync]   - %s\n", key)
	}

	// å°†é…ç½®æ•°æ®è½¬æ¢å›YAMLæ ¼å¼ï¼Œç”¨äºåŠ è½½ä¸»é…ç½®
	mainConfigYaml, err := yaml.Marshal(fullConfig)
	if err != nil {
		fmt.Printf("[data_sync] Failed to marshal config for main config: %v\n", err)
		return
	}

	// åŠ è½½ä¸»é…ç½®
	var cfg config.Config
	if err := yaml.Unmarshal(mainConfigYaml, &cfg); err != nil {
		fmt.Printf("[data_sync] Failed to parse main config: %v\n", err)
		return
	}
	config.ApplyProxy(&cfg)

	// é¢„åˆ›å»ºæ•°æ®åŒæ­¥æœåŠ¡ï¼ˆæ•°æ®åº“æš‚æ—¶ä¸ºnilï¼‰
	syncService := NewDataSyncService(nil, nil, &cfg)

	// åŠ è½½åŒæ­¥æœåŠ¡é…ç½®
	// ä»å·²è§£æçš„é…ç½®data_syncæ®µåŠ è½½
	configLoaded := false

	if dataSyncSection, exists := fullConfig["data_sync"]; exists {
		fmt.Printf("[data_sync] Found data_sync section in config\n")
		dataSyncBytes, err := yaml.Marshal(dataSyncSection)
		if err == nil {
			var syncCfg DataSyncConfig
			if err := yaml.Unmarshal(dataSyncBytes, &syncCfg); err == nil {
				// è°ƒè¯•ï¼šè¾“å‡ºè§£æåçš„é…ç½®
				fmt.Printf("[data_sync] YAMLä¸­åŒ…å«enable_realtime_gainers: %v\n", containsKey(dataSyncBytes, "enable_realtime_gainers"))

				// éªŒè¯é…ç½®
				if err := validateSyncConfig(&syncCfg); err != nil {
					fmt.Printf("[data_sync] Invalid sync config in main config: %v\n", err)
					return
				}

				syncService.config = syncCfg
				fmt.Printf("[data_sync] Loaded sync config from main config file: %s\n", *configPath)

				// è°ƒè¯•ï¼šè¾“å‡ºå®Œæ•´çš„åŠ è½½é…ç½®å†…å®¹
				configJson, _ := json.MarshalIndent(syncCfg, "", "  ")
				fmt.Printf("[data_sync] åŠ è½½çš„å®Œæ•´é…ç½®å†…å®¹:\n%s\n", string(configJson))

				configLoaded = true
			}
		}
	}

	if !configLoaded {
		fmt.Printf("[data_sync] Using default configuration\n")
	}

	// é…ç½®åŠ è½½å®Œæ¯•åï¼Œåˆå§‹åŒ–æ•°æ®åº“å’ŒæœåŠ¡
	// åˆå§‹åŒ–æ•°æ®åº“ï¼ˆä¼˜åŒ–è¿æ¥æ± é…ç½®ï¼‰
	database, err := pdb.OpenMySQL(pdb.Options{
		DSN:             cfg.Database.DSN,
		Automigrate:     true,
		MaxOpenConns:    20, // å¢åŠ è¿æ¥æ•°ä»¥æ”¯æŒå¹¶å‘åŒæ­¥
		MaxIdleConns:    10, // å¢åŠ ç©ºé—²è¿æ¥æ•°
		ConnMaxLifetime: 30 * time.Minute,
		ConnMaxIdleTime: 10 * time.Minute, // æ·»åŠ ç©ºé—²è¶…æ—¶
	})
	if err != nil {
		fmt.Printf("[data_sync] Failed to connect to database: %v\n", err)
		return
	}
	defer database.Close()

	gdb, err := database.DB()
	if err != nil {
		fmt.Printf("[data_sync] Failed to get database instance: %v\n", err)
		return
	}

	// è®¾ç½®æ•°æ®åº“è¿æ¥åˆ°å·²åˆ›å»ºçš„æœåŠ¡
	syncService.db = gdb

	// é‡æ–°åˆå§‹åŒ–ä¾èµ–æ•°æ®åº“çš„ç»„ä»¶
	syncService.initRedisClient()
	syncService.initSyncers()

	// æ³¨å†Œæ¡ä»¶åŒæ­¥å™¨ï¼ˆéœ€è¦åœ¨æ•°æ®åº“å’Œé…ç½®éƒ½å‡†å¤‡å¥½åè¿›è¡Œï¼‰
	if configLoaded {
		syncService.registerConditionalSyncers()
	}

	// æœ€ç»ˆé…ç½®éªŒè¯
	if err := validateSyncConfig(&syncService.config); err != nil {
		fmt.Printf("[data_sync] Final configuration validation failed: %v\n", err)
		return
	}

	// å¤„ç†ä¸åŒæ“ä½œ
	switch *action {
	case "test-sync":
		// æµ‹è¯•æ‰€æœ‰åŒæ­¥å™¨
		fmt.Println("[data_sync] Starting test sync for all syncers...")
		fmt.Println("[data_sync] This will test each syncer once and show detailed results")

		totalSyncers := len(syncService.syncers)
		successfulSyncers := 0

		for name, syncer := range syncService.syncers {
			fmt.Printf("[data_sync] Testing syncer: %s\n", name)
			startTime := time.Now()

			if err := syncer.Sync(syncService.ctx); err != nil {
				fmt.Printf("[data_sync] âŒ %s sync failed: %v\n", name, err)
			} else {
				duration := time.Since(startTime)
				fmt.Printf("[data_sync] âœ… %s sync succeeded in %v\n", name, duration)
				successfulSyncers++
			}

			// æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
			stats := syncer.GetStats()
			fmt.Printf("[data_sync]   Stats: %v\n", stats)
			fmt.Println()
		}

		fmt.Printf("[data_sync] Test sync completed: %d/%d syncers successful\n", successfulSyncers, totalSyncers)

		if successfulSyncers == totalSyncers {
			fmt.Println("[data_sync] ğŸ‰ All syncers are working correctly!")
		} else {
			fmt.Printf("[data_sync] âš ï¸  %d syncers have issues, check logs above\n", totalSyncers-successfulSyncers)
		}

		return

	case "start":
		// å¯åŠ¨æœåŠ¡
		if err := syncService.Start(*initialSyncMode); err != nil {
			fmt.Printf("[data_sync] Failed to start service: %v\n", err)
			return
		}

		// ç­‰å¾…ä¿¡å·
		sigChan := make(chan os.Signal, 1)
		signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)

		fmt.Println("[data_sync] Service started. Press Ctrl+C to stop.")

		<-sigChan
		fmt.Println("\n[data_sync] Received shutdown signal")

		syncService.Stop()

	case "sync-once":
		// å•æ¬¡åŒæ­¥
		if *syncerName == "" {
			fmt.Println("[data_sync] Error: syncer name is required for sync-once operation")
			fmt.Println("[data_sync] Available syncers: price, kline, futures, depth, market_stats, exchange_info, initial_gainers, realtime_gainers")
			fmt.Println("[data_sync] Example: -action sync-once -syncer price")
			return
		}

		fmt.Printf("[data_sync] Starting one-time sync for syncer: %s\n", *syncerName)
		startTime := time.Now()

		if err := syncService.SyncOnce(*syncerName); err != nil {
			fmt.Printf("[data_sync] âŒ Sync failed for %s: %v\n", *syncerName, err)
			os.Exit(1)
		} else {
			duration := time.Since(startTime)
			fmt.Printf("[data_sync] âœ… Sync completed successfully for %s in %v\n", *syncerName, duration)
		}

	case "status":
		// æŸ¥è¯¢çŠ¶æ€
		status := syncService.GetStatus()
		fmt.Printf("[data_sync] Service Status:\n")
		fmt.Printf("  Uptime: %v\n", status["uptime"])
		fmt.Printf("  Start Time: %v\n", status["start_time"])
		fmt.Printf("  Configured Symbols: %v\n", syncService.config.Symbols)
		fmt.Printf("  Total Symbols: %d\n", len(syncService.config.Symbols))

		if syncers, ok := status["syncers"].(map[string]interface{}); ok {
			fmt.Printf("  Syncers:\n")
			for name, stats := range syncers {
				fmt.Printf("    %s:\n", name)
				if statsMap, ok := stats.(map[string]interface{}); ok {
					for key, value := range statsMap {
						fmt.Printf("      %s: %v\n", key, value)
					}
				}
			}
		}

	default:
		fmt.Printf("[data_sync] Unknown action: %s\n", *action)
		fmt.Println("[data_sync] Available actions:")
		fmt.Println("[data_sync]   start     - å¯åŠ¨æ•°æ®åŒæ­¥æœåŠ¡")
		fmt.Println("[data_sync]   test-sync - æµ‹è¯•æ‰€æœ‰åŒæ­¥å™¨åŠŸèƒ½")
		fmt.Println("[data_sync]   sync-once - å•æ¬¡åŒæ­¥æŒ‡å®šåŒæ­¥å™¨")
		fmt.Println("[data_sync]   status    - æŸ¥çœ‹æœåŠ¡çŠ¶æ€")
		fmt.Println("[data_sync] Examples:")
		fmt.Println("[data_sync]   -action start")
		fmt.Println("[data_sync]   -action start -initial-sync-mode=skip")    // è·³è¿‡åˆå§‹åŒæ­¥æµ‹è¯•
		fmt.Println("[data_sync]   -action start -initial-sync-mode=random")  // éšæœºé¡ºåºæ‰§è¡Œåˆå§‹åŒæ­¥
		fmt.Println("[data_sync]   -action start -initial-sync-mode=ordered") // é¡ºåºæ‰§è¡Œåˆå§‹åŒæ­¥ï¼ˆé»˜è®¤ï¼‰
		fmt.Println("[data_sync]   -action test-sync")
		fmt.Println("[data_sync]   -action sync-once -syncer price")
		os.Exit(1)
	}
}

// å·¥å…·å‡½æ•°ï¼šè§£æå­—ç¬¦ä¸²æ•°ç»„
func parseStringArray(str string) []string {
	if str == "" {
		return nil
	}
	return strings.Split(str, ",")
}

// validateSyncConfig éªŒè¯åŒæ­¥é…ç½®çš„æœ‰æ•ˆæ€§
func validateSyncConfig(config *DataSyncConfig) error {
	// éªŒè¯äº¤æ˜“å¯¹ï¼ˆå¦‚æœé…ç½®äº†çš„è¯ï¼‰
	for _, symbol := range config.Symbols {
		if symbol == "" {
			return fmt.Errorf("empty symbol found in configuration")
		}
		// éªŒè¯äº¤æ˜“å¯¹æ ¼å¼ (åº”ä»¥USDTç»“å°¾)
		if !strings.HasSuffix(strings.ToUpper(symbol), "USDT") {
			return fmt.Errorf("invalid symbol format: %s (should end with USDT)", symbol)
		}
	}

	// éªŒè¯äº¤æ˜“æ‰€
	validExchanges := map[string]bool{"binance": true, "okx": true, "huobi": true}
	for _, exchange := range config.Exchanges {
		if !validExchanges[strings.ToLower(exchange)] {
			return fmt.Errorf("unsupported exchange: %s", exchange)
		}
	}

	// éªŒè¯Kçº¿é—´éš”
	validIntervals := map[string]bool{
		"1m": true, "3m": true, "5m": true, "15m": true, "30m": true,
		"1h": true, "2h": true, "4h": true, "6h": true, "8h": true, "12h": true,
		"1d": true, "3d": true, "1w": true, "1M": true,
	}
	for _, interval := range config.KlineIntervals {
		if !validIntervals[interval] {
			return fmt.Errorf("invalid kline interval: %s", interval)
		}
	}

	// éªŒè¯æ—¶é—´é—´éš”ï¼ˆæ”¯æŒå°æ•°ï¼Œå¦‚0.5è¡¨ç¤º30ç§’ï¼‰
	if config.PriceSyncInterval <= 0 || config.PriceSyncInterval > 3600 {
		return fmt.Errorf("invalid price sync interval: %.1f (must be 0.1-3600 minutes)", config.PriceSyncInterval)
	}
	if config.KlineSyncInterval <= 0 || config.KlineSyncInterval > 3600 {
		return fmt.Errorf("invalid kline sync interval: %.1f (must be 0.1-3600 minutes)", config.KlineSyncInterval)
	}
	if config.FuturesSyncInterval <= 0 || config.FuturesSyncInterval > 3600 {
		return fmt.Errorf("invalid futures sync interval: %.1f (must be 0.1-3600 minutes)", config.FuturesSyncInterval)
	}
	if config.FundingHistoryHours < 0 || config.FundingHistoryHours > 720 {
		return fmt.Errorf("invalid funding history hours: %d (must be 0-720 hours, 0 means use default 4 hours)", config.FundingHistoryHours)
	}
	if config.DepthSyncInterval <= 0 || config.DepthSyncInterval > 3600 {
		return fmt.Errorf("invalid depth sync interval: %.1f (must be 0.1-3600 minutes)", config.DepthSyncInterval)
	}
	if config.ExchangeInfoSyncInterval <= 0 || config.ExchangeInfoSyncInterval > 3600 {
		return fmt.Errorf("invalid exchange info sync interval: %.1f (must be 0.1-3600 minutes)", config.ExchangeInfoSyncInterval)
	}

	// éªŒè¯å…¶ä»–å‚æ•°
	if config.MaxRetries < 0 || config.MaxRetries > 10 {
		return fmt.Errorf("invalid max retries: %d (must be 0-10)", config.MaxRetries)
	}
	if config.BatchSize <= 0 || config.BatchSize > 1000 {
		return fmt.Errorf("invalid batch size: %d (must be 1-1000)", config.BatchSize)
	}

	return nil
}

// å·¥å…·å‡½æ•°ï¼šæ ¼å¼åŒ–æŒç»­æ—¶é—´
func formatDuration(d time.Duration) string {
	days := int(d.Hours() / 24)
	hours := int(d.Hours()) % 24
	minutes := int(d.Minutes()) % 60

	if days > 0 {
		return fmt.Sprintf("%dd%dh%dm", days, hours, minutes)
	}
	if hours > 0 {
		return fmt.Sprintf("%dh%dm", hours, minutes)
	}
	return fmt.Sprintf("%dm", minutes)
}

// startStatsUpdater å¯åŠ¨ç»Ÿè®¡ä¿¡æ¯æ›´æ–°å™¨
func (s *DataSyncService) startStatsUpdater() {
	s.statsUpdateTicker = time.NewTicker(30 * time.Second)
	go func() {
		for {
			select {
			case <-s.statsUpdateTicker.C:
				s.updateGlobalStats()
			case <-s.ctx.Done():
				s.statsUpdateTicker.Stop()
				return
			}
		}
	}()
}

// updateGlobalStats æ›´æ–°å…¨å±€ç»Ÿè®¡ä¿¡æ¯
func (s *DataSyncService) updateGlobalStats() {
	// ç”±äºDataSyncStatsçš„å­—æ®µæ˜¯ç§æœ‰çš„ï¼Œæˆ‘ä»¬é€šè¿‡AddAlertç­‰å‡½æ•°æ¥é—´æ¥æ›´æ–°
	// è¿™é‡Œæš‚æ—¶ä¸å®ç°å¤æ‚çš„ç»Ÿè®¡æ”¶é›†é€»è¾‘ï¼Œåç»­å¯ä»¥æ‰©å±•
	log.Printf("[DataSync] Stats update triggered (placeholder implementation)")
}

// getSyncerDisplayName è·å–åŒæ­¥å™¨æ˜¾ç¤ºåç§°
func (s *DataSyncService) getSyncerDisplayName(name string) string {
	names := map[string]string{
		"price":     "ä»·æ ¼åŒæ­¥å™¨",
		"kline":     "Kçº¿åŒæ­¥å™¨",
		"depth":     "æ·±åº¦åŒæ­¥å™¨",
		"websocket": "WebSocketåŒæ­¥å™¨",
	}
	if displayName, exists := names[name]; exists {
		return displayName
	}
	return name
}

// getSyncerNames è·å–åŒæ­¥å™¨åç§°åˆ—è¡¨
func getSyncerNames(syncers map[string]DataSyncer) []string {
	names := make([]string, 0, len(syncers))
	for name := range syncers {
		names = append(names, name)
	}
	return names
}

// containsKey æ£€æŸ¥YAMLæ•°æ®ä¸­æ˜¯å¦åŒ…å«æŒ‡å®šçš„é”®
func containsKey(yamlData []byte, key string) bool {
	return bytes.Contains(yamlData, []byte(key+":"))
}
